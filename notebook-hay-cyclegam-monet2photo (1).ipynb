{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Copyright 2019 The TensorFlow Authors.","metadata":{"id":"v1CUZ0dkOo_F"}},{"cell_type":"code","source":"#This notebook is based on the input pipeline of the Tensorflow Core model. In order to adjust the project to my own goals, I created my own parameters and cells (for example the generator and the discriminator) \n\n#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","metadata":{"cellView":"form","id":"qmkj-80IHxnd","execution":{"iopub.status.busy":"2022-09-11T15:31:55.467544Z","iopub.execute_input":"2022-09-11T15:31:55.467888Z","iopub.status.idle":"2022-09-11T15:31:55.566918Z","shell.execute_reply.started":"2022-09-11T15:31:55.467803Z","shell.execute_reply":"2022-09-11T15:31:55.489397Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# CycleGAN","metadata":{"id":"_xnMOsbqHz61"}},{"cell_type":"markdown","source":"<table class=\"tfo-notebook-buttons\" align=\"left\">\n  <td>\n    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/generative/cyclegan\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n  </td>\n  <td>\n    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/generative/cyclegan.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n  </td>\n</table>","metadata":{"id":"Ds4o1h4WHz9U"}},{"cell_type":"markdown","source":"This notebook demonstrates unpaired image to image translation using conditional GAN's, as described in [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593), also known as CycleGAN. The paper proposes a method that can capture the characteristics of one image domain and figure out how these characteristics could be translated into another image domain, all in the absence of any paired training examples. \n\nThis notebook assumes you are familiar with Pix2Pix, which you can learn about in the [Pix2Pix tutorial](https://www.tensorflow.org/tutorials/generative/pix2pix). The code for CycleGAN is similar, the main difference is an additional loss function, and the use of unpaired training data.\n\nCycleGAN uses a cycle consistency loss to enable training without the need for paired data. In other words, it can translate from one domain to another without a one-to-one mapping between the source and target domain. \n\nThis opens up the possibility to do a lot of interesting tasks like photo-enhancement, image colorization, style transfer, etc. All you need is the source and the target dataset (which is simply a directory of images).\n\n![Output Image 1](images/monet2photo_1.png)\n![Output Image 2](images/monet2photo_2.png)","metadata":{"id":"ITZuApL56Mny"}},{"cell_type":"markdown","source":"## Set up the input pipeline","metadata":{"id":"e1_Y75QXJS6h"}},{"cell_type":"code","source":"!pip install git+https://github.com/tensorflow/examples.git","metadata":{"id":"bJ1ROiQxJ-vY","execution":{"iopub.status.busy":"2022-09-11T15:31:55.568590Z","iopub.execute_input":"2022-09-11T15:31:55.569029Z","iopub.status.idle":"2022-09-11T15:32:12.066259Z","shell.execute_reply.started":"2022-09-11T15:31:55.568992Z","shell.execute_reply":"2022-09-11T15:32:12.065396Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow import keras","metadata":{"id":"lhSsUx9Nyb3t","execution":{"iopub.status.busy":"2022-09-11T15:42:55.018484Z","iopub.execute_input":"2022-09-11T15:42:55.018778Z","iopub.status.idle":"2022-09-11T15:42:55.023105Z","shell.execute_reply.started":"2022-09-11T15:42:55.018747Z","shell.execute_reply":"2022-09-11T15:42:55.022335Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import tensorflow_datasets as tfds\nfrom tensorflow_examples.models.pix2pix import pix2pix\n\nimport os\nimport time\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\nAUTOTUNE = tf.data.AUTOTUNE","metadata":{"id":"YfIk2es3hJEd","execution":{"iopub.status.busy":"2022-09-11T15:32:16.953825Z","iopub.execute_input":"2022-09-11T15:32:16.954079Z","iopub.status.idle":"2022-09-11T15:32:17.433892Z","shell.execute_reply.started":"2022-09-11T15:32:16.954045Z","shell.execute_reply":"2022-09-11T15:32:17.433118Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Input Pipeline\n\nAs mentioned in the [paper](https://arxiv.org/abs/1703.10593), apply random jittering and mirroring to the training dataset. These are some of the image augmentation techniques that avoids overfitting.\n\n\n* In random jittering, the image is resized to `286 x 286` and then randomly cropped to `256 x 256`.\n* In random mirroring, the image is randomly flipped horizontally i.e. left to right.","metadata":{"id":"iYn4MdZnKCey"}},{"cell_type":"code","source":"dataset, metadata = tfds.load('cycle_gan/monet2photo',\n                              with_info=True, as_supervised=True)\n\ntrain_monet, train_photo = dataset['trainA'], dataset['trainB']\ntest_monet, test_photo = dataset['testA'], dataset['testB']","metadata":{"id":"iuGVPOo7Cce0","execution":{"iopub.status.busy":"2022-09-11T15:32:17.436175Z","iopub.execute_input":"2022-09-11T15:32:17.436424Z","iopub.status.idle":"2022-09-11T15:32:32.992518Z","shell.execute_reply.started":"2022-09-11T15:32:17.436392Z","shell.execute_reply":"2022-09-11T15:32:32.991786Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"BUFFER_SIZE = 1000\nBATCH_SIZE = 1\nIMG_WIDTH = 256\nIMG_HEIGHT = 256","metadata":{"id":"2CbTEt448b4R","execution":{"iopub.status.busy":"2022-09-11T15:32:32.995633Z","iopub.execute_input":"2022-09-11T15:32:32.995904Z","iopub.status.idle":"2022-09-11T15:32:33.001579Z","shell.execute_reply.started":"2022-09-11T15:32:32.995873Z","shell.execute_reply":"2022-09-11T15:32:33.000491Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def random_crop(image):\n  cropped_image = tf.image.random_crop(\n      image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n\n  return cropped_image","metadata":{"id":"Yn3IwqhiIszt","execution":{"iopub.status.busy":"2022-09-11T15:32:33.002929Z","iopub.execute_input":"2022-09-11T15:32:33.003270Z","iopub.status.idle":"2022-09-11T15:32:33.010556Z","shell.execute_reply.started":"2022-09-11T15:32:33.003232Z","shell.execute_reply":"2022-09-11T15:32:33.009779Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# normalizing the images to [-1, 1]\ndef normalize(image):\n  image = tf.cast(image, tf.float32)\n  image = (image / 127.5) - 1\n  return image","metadata":{"id":"muhR2cgbLKWW","execution":{"iopub.status.busy":"2022-09-11T15:32:33.011851Z","iopub.execute_input":"2022-09-11T15:32:33.012232Z","iopub.status.idle":"2022-09-11T15:32:33.020987Z","shell.execute_reply.started":"2022-09-11T15:32:33.012198Z","shell.execute_reply":"2022-09-11T15:32:33.020195Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def random_jitter(image):\n  # resizing to 286 x 286 x 3\n  image = tf.image.resize(image, [286, 286],\n                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n  # randomly cropping to 256 x 256 x 3\n  image = random_crop(image)\n\n  # random mirroring\n  image = tf.image.random_flip_left_right(image)\n\n  return image","metadata":{"id":"fVQOjcPVLrUc","execution":{"iopub.status.busy":"2022-09-11T15:32:33.022081Z","iopub.execute_input":"2022-09-11T15:32:33.022409Z","iopub.status.idle":"2022-09-11T15:32:33.030447Z","shell.execute_reply.started":"2022-09-11T15:32:33.022376Z","shell.execute_reply":"2022-09-11T15:32:33.029785Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def preprocess_image_train(image, label):\n  image = random_jitter(image)\n  image = normalize(image)\n  return image","metadata":{"id":"tyaP4hLJ8b4W","execution":{"iopub.status.busy":"2022-09-11T15:32:33.032647Z","iopub.execute_input":"2022-09-11T15:32:33.032948Z","iopub.status.idle":"2022-09-11T15:32:33.051110Z","shell.execute_reply.started":"2022-09-11T15:32:33.032916Z","shell.execute_reply":"2022-09-11T15:32:33.050021Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def preprocess_image_test(image, label):\n  image = normalize(image)\n  return image","metadata":{"id":"VB3Z6D_zKSru","execution":{"iopub.status.busy":"2022-09-11T15:32:33.052751Z","iopub.execute_input":"2022-09-11T15:32:33.053590Z","iopub.status.idle":"2022-09-11T15:32:33.062398Z","shell.execute_reply.started":"2022-09-11T15:32:33.053549Z","shell.execute_reply":"2022-09-11T15:32:33.060909Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_monet = train_monet.cache().map(\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\n    BUFFER_SIZE).batch(BATCH_SIZE)\n\ntrain_photo = train_photo.cache().map(\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\n    BUFFER_SIZE).batch(BATCH_SIZE)\n\ntest_monet = test_monet.map(\n    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n    BUFFER_SIZE).batch(BATCH_SIZE)\n\ntest_photo = test_photo.map(\n    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n    BUFFER_SIZE).batch(BATCH_SIZE)","metadata":{"id":"RsajGXxd5JkZ","execution":{"iopub.status.busy":"2022-09-11T15:32:33.066259Z","iopub.execute_input":"2022-09-11T15:32:33.070856Z","iopub.status.idle":"2022-09-11T15:32:33.488924Z","shell.execute_reply.started":"2022-09-11T15:32:33.070812Z","shell.execute_reply":"2022-09-11T15:32:33.488194Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"sample_monet = next(iter(train_monet))\nsample_photo = next(iter(train_photo))","metadata":{"id":"e3MhJ3zVLPan","execution":{"iopub.status.busy":"2022-09-11T15:32:33.493297Z","iopub.execute_input":"2022-09-11T15:32:33.493946Z","iopub.status.idle":"2022-09-11T15:32:43.669280Z","shell.execute_reply.started":"2022-09-11T15:32:33.493908Z","shell.execute_reply":"2022-09-11T15:32:43.668490Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"plt.subplot(121)\nplt.title('monet')\nplt.imshow(sample_monet[0] * 0.5 + 0.5)\n\nplt.subplot(122)\nplt.title('monet with random jitter')\nplt.imshow(random_jitter(sample_monet[0]) * 0.5 + 0.5)","metadata":{"id":"4pOYjMk_KfIB","execution":{"iopub.status.busy":"2022-09-11T15:32:43.672913Z","iopub.execute_input":"2022-09-11T15:32:43.673547Z","iopub.status.idle":"2022-09-11T15:32:44.561144Z","shell.execute_reply.started":"2022-09-11T15:32:43.673496Z","shell.execute_reply":"2022-09-11T15:32:44.560397Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"plt.subplot(121)\nplt.title('photo')\nplt.imshow(sample_photo[0] * 0.5 + 0.5)\n\nplt.subplot(122)\nplt.title('photo with random jitter')\nplt.imshow(random_jitter(sample_photo[0]) * 0.5 + 0.5)","metadata":{"id":"0KJyB9ENLb2y","execution":{"iopub.status.busy":"2022-09-11T15:32:44.562133Z","iopub.execute_input":"2022-09-11T15:32:44.562422Z","iopub.status.idle":"2022-09-11T15:32:46.370275Z","shell.execute_reply.started":"2022-09-11T15:32:44.562392Z","shell.execute_reply":"2022-09-11T15:32:46.369591Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Generator and Discriminator","metadata":{"id":"hvX8sKsfMaio"}},{"cell_type":"markdown","source":"\n\n* Cyclegan uses [instance normalization](https://arxiv.org/abs/1607.08022) instead of [batch normalization](https://arxiv.org/abs/1502.03167).\n* The [CycleGAN paper](https://arxiv.org/abs/1703.10593) uses a modified `resnet` based generator.\n\nThere are 2 generators (G and F) and 2 discriminators (X and Y) being trained here. \n\nThe 'resnet' generator can be replaced by U-net generator architecture from tensorflow/examples (first command in the notebook).\n\n* Generator `G` learns to transform image `X` to image `Y`. $(G: X -> Y)$\n* Generator `F` learns to transform image `Y` to image `X`. $(F: Y -> X)$\n* Discriminator `D_X` learns to differentiate between image `X` and generated image `X` (`F(Y)`).\n* Discriminator `D_Y` learns to differentiate between image `Y` and generated image `Y` (`G(X)`).\n\n![Cyclegan model](images/cyclegan_model.png)","metadata":{"id":"cGrL73uCd-_M"}},{"cell_type":"code","source":"\ndef ResnetGenerator(input_shape,output_channels, dim,n_downsamplings,n_blocks,norm):\n    if norm  == 'instance_norm':\n         Norm = tfa.layers.InstanceNormalization\n    else:\n        Norm = keras.layers.BatchNormalization\n    \n\n    def _residual_block(x):\n        dim = x.shape[-1]\n        y = tf.pad(x, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')\n        y = keras.layers.Conv2D(dim, 3, padding='valid', use_bias=False)(y)\n        y = tf.nn.relu(Norm()(y))\n        y = tf.pad(y, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')\n        y = keras.layers.Conv2D(dim, 3, padding='valid', use_bias=False)(y)\n        y = Norm()(y)\n        return keras.layers.add([x, y])\n\n    \n    inputs = keras.Input(shape=input_shape)\n    z= inputs\n    z = tf.pad(z, [[0, 0], [3, 3], [3, 3], [0, 0]], mode='REFLECT')\n    z = keras.layers.Conv2D(dim, 7, padding='valid', use_bias=False)(z)  \n    z = tf.nn.relu(Norm()(z))\n\n    for _ in range(n_downsamplings):\n        dim *= 2\n        z = keras.layers.Conv2D(dim, 3, strides=2, padding='same', use_bias=False)(z)\n        z = tf.nn.relu(Norm()(z))\n        \n    for _ in range(n_blocks):\n        h = _residual_block(z)\n\n    for _ in range(n_downsamplings):\n        dim //= 2\n        z = keras.layers.Conv2DTranspose(dim, 3, strides=2, padding='same', use_bias=False)(z)\n        Norm()(z)\n        z = tf.nn.relu(Norm()(z))\n\n    z = tf.pad(h, [[0, 0], [3, 3], [3, 3], [0, 0]], mode='REFLECT')\n    z = keras.layers.Conv2D(output_channels, 7, padding='valid')(z)\n    z = tf.tanh(z)\n    \n    return keras.Model(inputs=inputs, outputs=z)","metadata":{"execution":{"iopub.status.busy":"2022-09-11T15:37:46.762234Z","iopub.execute_input":"2022-09-11T15:37:46.763013Z","iopub.status.idle":"2022-09-11T15:37:46.776482Z","shell.execute_reply.started":"2022-09-11T15:37:46.762975Z","shell.execute_reply":"2022-09-11T15:37:46.775651Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def Discriminator(input_shape,dim,n_downsamplings,norm):\n    dim_ = dim\n    if norm  == 'instance_norm':\n         Norm = tfa.layers.InstanceNormalization\n    else:\n        Norm = keras.layers.BatchNormalization\n\n    h = inputs = keras.Input(shape=input_shape)\n    h = keras.layers.Conv2D(dim, 4, strides=2, padding='same')(h)\n    h = tf.nn.leaky_relu(h, alpha=0.2)\n\n    for _ in range(n_downsamplings - 1):\n        dim = min(dim * 2, dim_ * 8)\n        h = keras.layers.Conv2D(dim, 4, strides=2, padding='same', use_bias=False)(h)\n        h = Norm()(h)\n        h = tf.nn.leaky_relu(h, alpha=0.2)\n\n    dim = min(dim * 2, dim_ * 8)\n    h = keras.layers.Conv2D(dim, 4, strides=1, padding='same', use_bias=False)(h)\n    h = Norm()(h)\n    h = tf.nn.leaky_relu(h, alpha=0.2)\n\n    h = keras.layers.Conv2D(1, 4, strides=1, padding='same')(h)\n\n    return keras.Model(inputs=inputs, outputs=h)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-11T15:37:54.344049Z","iopub.execute_input":"2022-09-11T15:37:54.344332Z","iopub.status.idle":"2022-09-11T15:37:54.353067Z","shell.execute_reply.started":"2022-09-11T15:37:54.344303Z","shell.execute_reply":"2022-09-11T15:37:54.352004Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"\n\ngenerator_g = ResnetGenerator(input_shape=(256, 256, 3),output_channels=3,dim=64,n_downsamplings=2,n_blocks=9,\n                              norm='instance_norm')\ngenerator_f = ResnetGenerator(input_shape=(256, 256, 3),output_channels=3,dim=64,n_downsamplings=2,n_blocks=9,\n                              norm='instance_norm')\n\ndiscriminator_x = Discriminator(input_shape=(256, 256, 3),dim=64, n_downsamplings=3,norm='instance_norm')\ndiscriminator_y = Discriminator(input_shape=(256, 256, 3),dim=64, n_downsamplings=3,norm='instance_norm')","metadata":{"id":"8ju9Wyw87MRW","execution":{"iopub.status.busy":"2022-09-11T15:43:45.081189Z","iopub.execute_input":"2022-09-11T15:43:45.081938Z","iopub.status.idle":"2022-09-11T15:43:46.699726Z","shell.execute_reply.started":"2022-09-11T15:43:45.081899Z","shell.execute_reply":"2022-09-11T15:43:46.699002Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"to_photo = generator_g(sample_monet)\nto_monet = generator_f(sample_photo)\nplt.figure(figsize=(8, 8))\ncontrast = 8\n\nimgs = [sample_monet, to_photo, sample_photo, to_monet]\ntitle = ['monet', 'To photo', 'photo', 'To monet']\n\nfor i in range(len(imgs)):\n  plt.subplot(2, 2, i+1)\n  plt.title(title[i])\n  if i % 2 == 0:\n    plt.imshow(imgs[i][0] * 0.5 + 0.5)\n  else:\n    plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)\nplt.show()","metadata":{"id":"wDaGZ3WpZUyw","execution":{"iopub.status.busy":"2022-09-11T15:43:46.737613Z","iopub.execute_input":"2022-09-11T15:43:46.738210Z","iopub.status.idle":"2022-09-11T15:43:54.394112Z","shell.execute_reply.started":"2022-09-11T15:43:46.738179Z","shell.execute_reply":"2022-09-11T15:43:54.391845Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 8))\n\nplt.subplot(121)\nplt.title('Is a real photo?')\nplt.imshow(discriminator_y(sample_photo)[0, ..., -1], cmap='RdBu_r')\n\nplt.subplot(122)\nplt.title('Is a real monet?')\nplt.imshow(discriminator_x(sample_monet)[0, ..., -1], cmap='RdBu_r')\n\nplt.show()","metadata":{"id":"O5MhJmxyZiy9","execution":{"iopub.status.busy":"2022-09-11T15:44:00.821732Z","iopub.execute_input":"2022-09-11T15:44:00.822518Z","iopub.status.idle":"2022-09-11T15:44:01.394971Z","shell.execute_reply.started":"2022-09-11T15:44:00.822476Z","shell.execute_reply":"2022-09-11T15:44:01.394234Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Loss functions","metadata":{"id":"0FMYgY_mPfTi"}},{"cell_type":"markdown","source":"In CycleGAN, there is no paired data to train on, hence there is no guarantee that the input `x` and the target `y` pair are meaningful during training. Thus in order to enforce that the network learns the correct mapping, the authors propose the cycle consistency loss.\n","metadata":{"id":"JRqt02lupRn8"}},{"cell_type":"code","source":"LAMBDA = 10","metadata":{"id":"cyhxTuvJyIHV","execution":{"iopub.status.busy":"2022-09-11T15:44:05.845088Z","iopub.execute_input":"2022-09-11T15:44:05.846910Z","iopub.status.idle":"2022-09-11T15:44:05.851231Z","shell.execute_reply.started":"2022-09-11T15:44:05.846857Z","shell.execute_reply":"2022-09-11T15:44:05.850520Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)","metadata":{"id":"Q1Xbz5OaLj5C","execution":{"iopub.status.busy":"2022-09-11T15:44:07.229404Z","iopub.execute_input":"2022-09-11T15:44:07.229673Z","iopub.status.idle":"2022-09-11T15:44:07.235927Z","shell.execute_reply.started":"2022-09-11T15:44:07.229644Z","shell.execute_reply":"2022-09-11T15:44:07.235240Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def discriminator_loss(real, generated):\n  real_loss = loss_obj(tf.ones_like(real), real)\n\n  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n\n  total_disc_loss = real_loss + generated_loss\n\n  return total_disc_loss * 0.5","metadata":{"id":"wkMNfBWlT-PV","execution":{"iopub.status.busy":"2022-09-11T15:44:08.630670Z","iopub.execute_input":"2022-09-11T15:44:08.631318Z","iopub.status.idle":"2022-09-11T15:44:08.636241Z","shell.execute_reply.started":"2022-09-11T15:44:08.631280Z","shell.execute_reply":"2022-09-11T15:44:08.635498Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def generator_loss(generated):\n  return loss_obj(tf.ones_like(generated), generated)","metadata":{"id":"90BIcCKcDMxz","execution":{"iopub.status.busy":"2022-09-11T15:44:15.823145Z","iopub.execute_input":"2022-09-11T15:44:15.823819Z","iopub.status.idle":"2022-09-11T15:44:15.829891Z","shell.execute_reply.started":"2022-09-11T15:44:15.823782Z","shell.execute_reply":"2022-09-11T15:44:15.829008Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"Cycle consistency means the result should be close to the original input. For example, if one translates a sentence from English to French, and then translates it back from French to English, then the resulting sentence should be the same as the  original sentence.\n\nIn cycle consistency loss, \n\n* Image $X$ is passed via generator $G$ that yields generated image $\\hat{Y}$.\n* Generated image $\\hat{Y}$ is passed via generator $F$ that yields cycled image $\\hat{X}$.\n* Mean absolute error is calculated between $X$ and $\\hat{X}$.\n\n$$forward\\ cycle\\ consistency\\ loss: X -> G(X) -> F(G(X)) \\sim \\hat{X}$$\n\n$$backward\\ cycle\\ consistency\\ loss: Y -> F(Y) -> G(F(Y)) \\sim \\hat{Y}$$\n\n\n![Cycle loss](images/cycle_loss.png)","metadata":{"id":"5iIWQzVF7f9e"}},{"cell_type":"code","source":"def calc_cycle_loss(real_image, cycled_image):\n  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n  \n  return LAMBDA * loss1","metadata":{"id":"NMpVGj_sW6Vo","execution":{"iopub.status.busy":"2022-09-11T15:44:18.567163Z","iopub.execute_input":"2022-09-11T15:44:18.567471Z","iopub.status.idle":"2022-09-11T15:44:18.571619Z","shell.execute_reply.started":"2022-09-11T15:44:18.567438Z","shell.execute_reply":"2022-09-11T15:44:18.570934Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"As shown above, generator $G$ is responsible for translating image $X$ to image $Y$. Identity loss says that, if you fed image $Y$ to generator $G$, it should yield the real image $Y$ or something close to image $Y$.\n\nIf you run the photo-to-monet model on a monet or the monet-to-photo model on a photo, it should not modify the image much since the image already contains the target class.\n\n$$Identity\\ loss = |G(Y) - Y| + |F(X) - X|$$","metadata":{"id":"U-tJL-fX0Mq7"}},{"cell_type":"code","source":"def identity_loss(real_image, same_image):\n  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n  return LAMBDA * 0.5 * loss","metadata":{"id":"05ywEH680Aud","execution":{"iopub.status.busy":"2022-09-11T15:44:30.010340Z","iopub.execute_input":"2022-09-11T15:44:30.010838Z","iopub.status.idle":"2022-09-11T15:44:30.014971Z","shell.execute_reply.started":"2022-09-11T15:44:30.010800Z","shell.execute_reply":"2022-09-11T15:44:30.014219Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"Initialize the optimizers for all the generators and the discriminators.","metadata":{"id":"G-vjRM7IffTT"}},{"cell_type":"code","source":"generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ngenerator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\ndiscriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"id":"iWCn_PVdEJZ7","execution":{"iopub.status.busy":"2022-09-11T15:44:34.626177Z","iopub.execute_input":"2022-09-11T15:44:34.626994Z","iopub.status.idle":"2022-09-11T15:44:34.632689Z","shell.execute_reply.started":"2022-09-11T15:44:34.626935Z","shell.execute_reply":"2022-09-11T15:44:34.631950Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## Checkpoints","metadata":{"id":"aKUZnDiqQrAh"}},{"cell_type":"code","source":"checkpoint_path = \"./checkpoints/train\"\n\nckpt = tf.train.Checkpoint(generator_g=generator_g,\n                           generator_f=generator_f,\n                           discriminator_x=discriminator_x,\n                           discriminator_y=discriminator_y,\n                           generator_g_optimizer=generator_g_optimizer,\n                           generator_f_optimizer=generator_f_optimizer,\n                           discriminator_x_optimizer=discriminator_x_optimizer,\n                           discriminator_y_optimizer=discriminator_y_optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n  ckpt.restore(ckpt_manager.latest_checkpoint)\n  print ('Latest checkpoint restored!!')","metadata":{"id":"WJnftd5sQsv6","execution":{"iopub.status.busy":"2022-09-11T15:44:37.549621Z","iopub.execute_input":"2022-09-11T15:44:37.550245Z","iopub.status.idle":"2022-09-11T15:44:37.556919Z","shell.execute_reply.started":"2022-09-11T15:44:37.550208Z","shell.execute_reply":"2022-09-11T15:44:37.556169Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\nNote: This example model is trained for fewer epochs (10) than the paper (200) to keep training time reasonable . The generated images will have much lower quality.","metadata":{"id":"Rw1fkAczTQYh"}},{"cell_type":"code","source":"EPOCHS = 10","metadata":{"id":"NS2GWywBbAWo","execution":{"iopub.status.busy":"2022-09-11T15:44:44.596003Z","iopub.execute_input":"2022-09-11T15:44:44.596278Z","iopub.status.idle":"2022-09-11T15:44:44.600250Z","shell.execute_reply.started":"2022-09-11T15:44:44.596248Z","shell.execute_reply":"2022-09-11T15:44:44.599309Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def generate_images(model, test_input):\n  prediction = model(test_input)\n    \n  plt.figure(figsize=(12, 12))\n\n  display_list = [test_input[0], prediction[0]]\n  title = ['Input Image', 'Predicted Image']\n\n  for i in range(2):\n    plt.subplot(1, 2, i+1)\n    plt.title(title[i])\n    # getting the pixel values between [0, 1] to plot it.\n    plt.imshow(display_list[i] * 0.5 + 0.5)\n    plt.axis('off')\n  plt.show()","metadata":{"id":"RmdVsmvhPxyy","execution":{"iopub.status.busy":"2022-09-11T15:44:46.129072Z","iopub.execute_input":"2022-09-11T15:44:46.129762Z","iopub.status.idle":"2022-09-11T15:44:46.135349Z","shell.execute_reply.started":"2022-09-11T15:44:46.129691Z","shell.execute_reply":"2022-09-11T15:44:46.134628Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"Even though the training loop looks complicated, it consists of four basic steps:\n\n* Get the predictions.\n* Calculate the loss.\n* Calculate the gradients using backpropagation.\n* Apply the gradients to the optimizer.","metadata":{"id":"kE47ERn5fyLC"}},{"cell_type":"code","source":"@tf.function\ndef train_step(real_x, real_y):\n  # persistent is set to True because the tape is used more than\n  # once to calculate the gradients.\n  with tf.GradientTape(persistent=True) as tape:\n    # Generator G translates X -> Y\n    # Generator F translates Y -> X.\n    \n    fake_y = generator_g(real_x, training=True)\n    cycled_x = generator_f(fake_y, training=True)\n\n    fake_x = generator_f(real_y, training=True)\n    cycled_y = generator_g(fake_x, training=True)\n\n    # same_x and same_y are used for identity loss.\n    same_x = generator_f(real_x, training=True)\n    same_y = generator_g(real_y, training=True)\n\n    disc_real_x = discriminator_x(real_x, training=True)\n    disc_real_y = discriminator_y(real_y, training=True)\n\n    disc_fake_x = discriminator_x(fake_x, training=True)\n    disc_fake_y = discriminator_y(fake_y, training=True)\n\n    # calculate the loss\n    gen_g_loss = generator_loss(disc_fake_y)\n    gen_f_loss = generator_loss(disc_fake_x)\n    \n    total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n    \n    # Total generator loss = adversarial loss + cycle loss\n    total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n    total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n\n    disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n    disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n  \n  # Calculate the gradients for generator and discriminator\n  generator_g_gradients = tape.gradient(total_gen_g_loss, \n                                        generator_g.trainable_variables)\n  generator_f_gradients = tape.gradient(total_gen_f_loss, \n                                        generator_f.trainable_variables)\n  \n  discriminator_x_gradients = tape.gradient(disc_x_loss, \n                                            discriminator_x.trainable_variables)\n  discriminator_y_gradients = tape.gradient(disc_y_loss, \n                                            discriminator_y.trainable_variables)\n  \n  # Apply the gradients to the optimizer\n  generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n                                            generator_g.trainable_variables))\n\n  generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n                                            generator_f.trainable_variables))\n  \n  discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n                                                discriminator_x.trainable_variables))\n  \n  discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n                                                discriminator_y.trainable_variables))","metadata":{"id":"KBKUV2sKXDbY","execution":{"iopub.status.busy":"2022-09-11T15:44:52.783338Z","iopub.execute_input":"2022-09-11T15:44:52.783609Z","iopub.status.idle":"2022-09-11T15:44:52.794944Z","shell.execute_reply.started":"2022-09-11T15:44:52.783581Z","shell.execute_reply":"2022-09-11T15:44:52.794140Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n  start = time.time()\n\n  n = 0\n  for image_x, image_y in tf.data.Dataset.zip((train_monet, train_photo)):\n    train_step(image_x, image_y)\n    if n % 10 == 0:\n      print ('.', end='')\n    n += 1\n\n  clear_output(wait=True)\n  # Using a consistent image (sample_monet) so that the progress of the model\n  # is clearly visible.\n  generate_images(generator_g, sample_monet)\n\n  if (epoch + 1) % 5 == 0:\n    ckpt_save_path = ckpt_manager.save()\n    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n                                                         ckpt_save_path))\n\n  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n                                                      time.time()-start))","metadata":{"id":"2M7LmLtGEMQJ","execution":{"iopub.status.busy":"2022-09-11T15:45:00.693399Z","iopub.execute_input":"2022-09-11T15:45:00.693736Z","iopub.status.idle":"2022-09-11T16:49:44.353253Z","shell.execute_reply.started":"2022-09-11T15:45:00.693683Z","shell.execute_reply":"2022-09-11T16:49:44.352418Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"## Generate using test dataset","metadata":{"id":"1RGysMU_BZhx"}},{"cell_type":"code","source":"# Run the trained model on the test dataset\nfor inp in test_photo.take(5):\n  generate_images(generator_f, inp)","metadata":{"id":"KUgSnmy2nqSP","execution":{"iopub.status.busy":"2022-09-11T16:49:44.354895Z","iopub.execute_input":"2022-09-11T16:49:44.355542Z","iopub.status.idle":"2022-09-11T16:49:47.574296Z","shell.execute_reply.started":"2022-09-11T16:49:44.355503Z","shell.execute_reply":"2022-09-11T16:49:47.573451Z"},"trusted":true},"execution_count":43,"outputs":[]}]}